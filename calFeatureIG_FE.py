import numpy as np
import pandas as pd
from collections import *
import cPickle
import time

def calEnt(rawList):
    length = float(len(rawList))
    c = Counter(rawList)
    distribution = [v/length for v in c.values()]
    ent = 0
    for p in distribution:
        ent -= p * np.log2(p)
    return ent

def calEntFromDistributionList(distribution):
    ent = 0
    for p in distribution:
        ent -= p * np.log2(p)
    return ent

def getClassFromId(id):
    # return opngramMap[id]['g']
    return label[label.Id == id].reset_index().loc[0, 'Class']

def getOpngram(opcodelist, n=3):
    opngramlist = [tuple(opcodelist[i:i+n]) for i in range(len(opcodelist)-n)]
    opngram = Counter(opngramlist)
    return opngram

# '''
baseDir = "E:/msmalware/trainops/"
label = pd.read_csv("E:/msmalware/subtrain200labels.csv")
# get ops directly from ops file (malware)
opngramMap = {}
count = 1
for id in label.Id:
    filepath = baseDir + id + ".txt"
    f = open(filepath)
    opseq = []
    for line in f:
        opseq.append(line[:-1])
    opngram = getOpngram(opseq)
    print "counting ngram of", count, "file...", opngram
    count += 1
    opngramMap[id] = opngram
# '''

# opngramMap = cPickle.load(open("E:/sub40v2opngram.p", "rb"))
# print "type of opngramMap:", opngramMap.__class__

'''interface: opngramMap'''

# sumc = Counter()
# for v in opngramMap.values():
#     sumc += v
# features = sumc.keys()

features = set()
for v in opngramMap.values():
    features.update(v.keys())

l = len(features)
steps = [1, 5, 10, 20]
for step in steps:
    f = open("E:/log.txt", "a")
    print >> f, ""
    print >> f, time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(time.time()))
    print >> f, "count distribution entropy on every feature of", l, "features, sub200", "step=", step
    f.close()

    '''interface: opngramMap, features'''

    d = opngramMap.keys()
    '''
    draw = []
    for id in d:
        draw.append(getClassFromId(id))
    dent = calEnt(draw)
    print "dent:", dent
    featureInfoIncreMap = {}
    count = 1
    '''

    '''
    Each feature has a Raw Map, the Map means grouping the data by this feature.
    Id List: (feature-value=v1): [2,3,7,8,9,15]
    Raw list: (the classes of ids in Id list): [1,1,1,1,0,0]
    distribution list: [2/3, 1/3]

    then the entropy can be calculated
    '''

    # calculate information gain
    '''
    for feature in features:
        print "counting information increase of feature", count, "in", l, "features..."
        count += 1
        partitionRawMap = defaultdict(list)
        for id in d:
            partitionRawMap[opngramMap[id][feature] / step].append(getClassFromId(id))

        partitionEntMap = {}
        for ftype, raw in partitionRawMap.iteritems():
            partitionEntMap[ftype] = tuple([calEnt(raw), len(raw) / float(len(d))])
        sumEnt = 0.0
        for v in partitionEntMap.values():
            sumEnt += np.cumprod(v)[-1]  # v[0]*v[1]
        featureInfoIncreMap[feature] = dent - sumEnt
    '''

    # calculate distribution entropy on every feature (using it calculate information gain ratio)
    dlen = float(len(opngramMap))
    featureEntMap = {}
    count = 1
    for feature in features:
        print "counting distribution entropy of feature", count, "in", l, "features...", "step=", step
        count += 1
        partitionNumMap = defaultdict(int)
        for id in d:
            partitionNumMap[opngramMap[id][feature]] += 1
        distribution = partitionNumMap.values()
        distribution = [v / dlen for v in distribution]
        featureEntMap[feature] = calEntFromDistributionList(distribution)

    '''result: featureInfoIncreMap'''
    f = open("E:/log.txt", "a")
    print >> f, time.strftime("End time: %Y-%m-%d %H:%M:%S", time.localtime(time.time()))
    f.close()

    df = pd.DataFrame(featureEntMap.items(), columns=['ngram', 'featureEnt'])
    df.to_csv("E:/sub200FE_step"+str(step)+".csv", index=False)

    cPickle.dump(featureEntMap, open("E:/sub200FE_step"+str(step)+".p", "wb"))
